{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6630da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guusl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\guusl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import linear_model\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878841d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON files\n",
    "\n",
    "f = open('train.json')\n",
    "data = json.load(f)\n",
    "\n",
    "g = open('test.json', )\n",
    "test = json.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d4f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame(ds, additional_cols=[]):\n",
    "    df = pd.DataFrame(ds)\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['labels'] = df[['title', 'abstract', 'year', 'venue']].apply(lambda x: ','.join(x), axis=1)\n",
    "\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    merged = df.drop(labels=['title', 'abstract', 'venue'] + additional_cols, axis=1)\n",
    "    merged.head()\n",
    "\n",
    "    # This replaces capital letters, and some symbols. It lowers all the texts, strips and splits and converts it to strings\n",
    "    labels = merged['labels'].str.replace('[^A-Za-z]', ' ').str.lower().str.strip().str.split()\n",
    "    merged['labels'] = [','.join(map(str, l)) for l in labels]\n",
    "\n",
    "    return df, merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6ee4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>year</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>2014</td>\n",
       "      <td>detecting,linguistic,idiosyncratic,interests,i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  authorId         authorName  year  \\\n",
       "0  3188285  Masoud Rouhizadeh  2014   \n",
       "\n",
       "                                              labels  \n",
       "0  detecting,linguistic,idiosyncratic,interests,i...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing training/validation data\n",
    "# It combines the columns that are given in the original dataset, to have only one independent variable column\n",
    "data, merged_data = createDataFrame(data, ['paperId'])\n",
    "merged_data.describe()\n",
    "merged_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa8af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing test data (same process as above, but for test dataset)\n",
    "test, merged_test = createDataFrame(test, ['paperId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37dcccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>year</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>2014</td>\n",
       "      <td>detecting,linguistic,idiosyncratic,interests,i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2782720</td>\n",
       "      <td>Yuri Bizzoni</td>\n",
       "      <td>2018</td>\n",
       "      <td>bigrams,and,bilstms,two,neural,networks,for,se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>144748442</td>\n",
       "      <td>Peter Vickers</td>\n",
       "      <td>2021</td>\n",
       "      <td>in,factuality,efficient,integration,of,relevan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46331602</td>\n",
       "      <td>Irene Li</td>\n",
       "      <td>2022</td>\n",
       "      <td>variational,graph,autoencoding,as,cheap,superv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30887404</td>\n",
       "      <td>Junru Zhou</td>\n",
       "      <td>2019</td>\n",
       "      <td>limit,bert,linguistics,informed,multi,task,ber...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    authorId         authorName  year  \\\n",
       "0    3188285  Masoud Rouhizadeh  2014   \n",
       "1    2782720       Yuri Bizzoni  2018   \n",
       "2  144748442      Peter Vickers  2021   \n",
       "3   46331602           Irene Li  2022   \n",
       "4   30887404         Junru Zhou  2019   \n",
       "\n",
       "                                              labels  \n",
       "0  detecting,linguistic,idiosyncratic,interests,i...  \n",
       "1  bigrams,and,bilstms,two,neural,networks,for,se...  \n",
       "2  in,factuality,efficient,integration,of,relevan...  \n",
       "3  variational,graph,autoencoding,as,cheap,superv...  \n",
       "4  limit,bert,linguistics,informed,multi,task,ber...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4fe2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace stopwords, symbols\n",
    "# cleans out all the stopwords (the, a, an, etc.)\n",
    "# removes all symbols and numbers\n",
    "\n",
    "replace = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "replace_symbols = re.compile('[^0-9a-z #+_]')\n",
    "replace_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_labels(labels):\n",
    "    labels = labels.lower()  # lowercase labels\n",
    "    labels = replace.sub(' ', labels)  # replace REPLACE_BY_SPACE_RE symbols by space in labels\n",
    "    labels = replace_symbols.sub('', labels)  # delete symbols which are in BAD_SYMBOLS_RE from labels\n",
    "    labels = ' '.join(word for word in labels.split() if word not in replace_stopwords)  # delete stopwords from labels\n",
    "    return labels\n",
    "\n",
    "merged_data['labels'] = merged_data['labels'].apply(clean_labels)  # applies the above loop to the data\n",
    "merged_test['labels'] = merged_test['labels'].apply(clean_labels)  # applies the loop to test data\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()  # seperates the sentences to get single words\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()  # lemmatizes text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "merged_data['labels'] = merged_data['labels'].apply(lemmatize_text)\n",
    "merged_data['labels'] = [','.join(map(str, l)) for l in merged_data['labels']]\n",
    "\n",
    "merged_test['labels'] = merged_test['labels'].apply(lemmatize_text)\n",
    "merged_test['labels'] = [','.join(map(str, l)) for l in merged_test['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47d022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['labels'] = merged_data.labels.apply(lambda x: ', '.join(i[0] for i in Counter(x).most_common(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7140f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ,, i, e, t, a\n",
       "1        ,, e, t, r, n\n",
       "2        ,, e, a, t, i\n",
       "3        ,, e, a, r, n\n",
       "4        ,, e, t, i, n\n",
       "             ...      \n",
       "12124    ,, e, a, t, r\n",
       "12125    ,, e, a, t, r\n",
       "12126    ,, e, a, t, n\n",
       "12127    ,, e, a, i, t\n",
       "12128    e, ,, r, i, n\n",
       "Name: labels, Length: 12129, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c7fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.sample(100000, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc0188da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>,, t, e, a, i</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>,, a, e, t, n</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>e, ,, a, t, n</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>,, a, e, t, i</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>,, e, t, n, a</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             labels  year\n",
       "1497  ,, t, e, a, i  2020\n",
       "5786  ,, a, e, t, n  2015\n",
       "4307  e, ,, a, t, n  2021\n",
       "182   ,, a, e, t, i  2019\n",
       "4453  ,, e, t, n, a  2015"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine independent and dependent variables \n",
    "\n",
    "X = merged_data[['labels', 'year']]\n",
    "y = merged_data['authorId']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9adf9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(ngram_range=(1,2), n_features=2**18)\n",
    "\n",
    "# applies the hashing vectorizer (found to be the best for large text datasets)\n",
    "X_train_hashed = vectorizer.transform(X_train['labels'])\n",
    "tfidf_transformer=TfidfTransformer() # transforms the hashed vectorized texted by using tfidf transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c3cf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the final dataframe shape and data for the training and testing phases\n",
    "def transformFrame(frame, fit = False):\n",
    "    labels = frame['labels']\n",
    "    hashed = vectorizer.fit_transform(labels) if fit else vectorizer.transform(labels)\n",
    "    transformed = pd.DataFrame.sparse.from_spmatrix(hashed)\n",
    "    # TODO: Can we keep the year in the resulting DF (transformed)?\n",
    "    return transformed # df.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca64bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF works by proportionally increasing the number of times a word appears in the document\n",
    "# but is counterbalanced by the number of documents in which it is present\n",
    "# https://www.analyticsvidhya.com/blog/2021/07/bag-of-words-vs-tfidf-vectorization-a-hands-on-tutorial/\n",
    "X_train_trans = transformFrame(X_train, True)\n",
    "X_test_trans = transformFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67d95226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>262134</th>\n",
       "      <th>262135</th>\n",
       "      <th>262136</th>\n",
       "      <th>262137</th>\n",
       "      <th>262138</th>\n",
       "      <th>262139</th>\n",
       "      <th>262140</th>\n",
       "      <th>262141</th>\n",
       "      <th>262142</th>\n",
       "      <th>262143</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 262144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   9       ...  262134  262135  262136  262137  262138  262139  262140  \\\n",
       "0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   262141  262142  262143  \n",
       "0     0.0     0.0     0.0  \n",
       "1     0.0     0.0     0.0  \n",
       "2     0.0     0.0     0.0  \n",
       "3     0.0     0.0     0.0  \n",
       "4     0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 262144 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8f5af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1497        2991702\n",
       "5786     1402921027\n",
       "4307       51199981\n",
       "182         2216545\n",
       "4453        2544049\n",
       "            ...    \n",
       "580         1990190\n",
       "1885       22603977\n",
       "11737      32251567\n",
       "11643       2023469\n",
       "2583       51229603\n",
       "Name: authorId, Length: 85000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ba8b7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "# this is the part where the hyperparameter tuning happens\n",
    "\n",
    "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "l1 = np.arange(0,1,0.01)\n",
    "l2 = np.arange(0,1,0.01)\n",
    "class_weight = [{0:i,1:j} for i,j in zip(l1,l2)]\n",
    "eta0 = [0.1, 1, 10, 100]\n",
    "\n",
    "# TODO Can we optimize performance? This is slow\n",
    "param_distributions = dict(loss=loss,\n",
    "                           penalty=penalty,\n",
    "                           alpha=alpha,\n",
    "                           learning_rate=learning_rate,\n",
    "                           eta0=eta0)\n",
    "sgd = linear_model.SGDClassifier()\n",
    "random = RandomizedSearchCV(estimator=sgd, param_distributions=param_distributions, verbose=1, n_jobs=-1, n_iter=30, cv=3)\n",
    "random_result = random.fit(X_train_trans, Y_train)\n",
    "\n",
    "print('Best Score: ', random_result.best_score_)\n",
    "print('Best Params: ', random_result.best_params_)\n",
    "\n",
    "# https://www.kaggle.com/code/tboyle10/hyperparameter-tuning\n",
    "\n",
    "# dict_keys(['alpha', 'average', 'class_weight', 'early_stopping', 'epsilon', 'eta0', 'fit_intercept', \n",
    "# 'l1_ratio', 'learning_rate', 'loss', 'max_iter', 'n_iter_no_change', 'n_jobs', 'penalty', 'power_t', \n",
    "# 'random_state', 'shuffle', 'tol', 'validation_fraction', 'verbose', 'warm_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b21384",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This trains the model\n",
    "\n",
    "epoch = 1 # the amount of times that it will run\n",
    "batchsize = 1500 # the amount of data you put in per batch\n",
    "model = SGDClassifier() # The classifier to use, this is supposed to be best applicable to large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc261327",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "batches = int(X_train_trans.shape[0]/batchsize) + 1\n",
    "print(\"Determined batches\", len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d436a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "samples = X_train_trans.shape[0]\n",
    "print(\"Determined samples\", len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e865099",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    for j in range(batches):\n",
    "        print('in j...', j, j*batchsize, '----2is:',samples, (j+1)*batchsize )\n",
    "        model.partial_fit(X_train_trans[j*batchsize:min(samples,(j+1)*batchsize)],\n",
    "                          Y_train[j*batchsize:min(samples,(j+1)*batchsize)],\n",
    "                          classes=np.unique(y))\n",
    "print (\"Accuracy on testing data :\", model.score(X_test_trans, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f17b78",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "# https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py\n",
    "# https://medium.com/mlearning-ai/out-of-core-multi-label-text-classification-with-scikit-learn-14afa4c1bb75\n",
    "# https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e735b96",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "new_data = merged_test['labels'] # the test data labels that we will apply to transform to numerical\n",
    "X_new = vectorizer.transform(new_data) # transforms the data by using the vectorizer\n",
    "y_pred = model.predict(X_new) # predicts the new values of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121498a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "merged_test['prediction'] = y_pred.tolist() # puts the predicted data to list\n",
    "final = merged_test.set_axis(['paperId', 'labels', 'authorId'], axis=1, inplace=False) # changes the axis labels\n",
    "final = final.drop(labels = ['labels'], axis = 1) # drops the labels column to get final result of only paperId & authorId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d426a8b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13491d97",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "merged_data.loc[merged_data['authorId'] == '1747849']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692eb9d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# To get the file into the predicted.json file required by teachers\n",
    "#output = final.to_dict(orient='records')\n",
    "#jsonString = json.dumps(output)\n",
    "#jsonFile = open('predicted.json', 'w')\n",
    "#jsonFile.write(jsonString)\n",
    "#jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08c009",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
