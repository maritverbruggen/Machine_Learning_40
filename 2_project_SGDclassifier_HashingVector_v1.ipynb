{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1a6630da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guusl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\guusl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "27d4f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction model using CountVectorizer() and LogisticRegression(), which is insufficient to train all data from training set\n",
    "\n",
    "# Opening JSON files\n",
    "\n",
    "f = open('train.json')\n",
    "data = json.load(f)\n",
    "\n",
    "g = open('test.json', )\n",
    "test = json.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "def createDataFrame(ds, additional_cols=[]):\n",
    "    df = pd.DataFrame(ds)\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['labels'] = df[['title', 'abstract', 'year', 'venue']].apply(lambda x: ','.join(x), axis=1)\n",
    "    merged = df.drop(labels=['title', 'abstract', 'year', 'venue'] + additional_cols, axis=1)\n",
    "    merged.head()\n",
    "\n",
    "    # This replaces capital letters, and some symbols. It lowers all the texts, strips and splits and converts it to strings\n",
    "    labels = merged['labels'].str.replace('[^A-Za-z]', ' ').str.lower().str.strip().str.split()\n",
    "    merged['labels'] = [','.join(map(str, l)) for l in labels]\n",
    "\n",
    "    return df, merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "data": {
      "text/plain": "                                    paperId  \\\n0  0b341b6938308a6d5f47edf490f6e46eae3835fa   \n1  c682727ee058aadbe9dbf838dcb036322818f588   \n2  0f9b5b32229a7245e43754430c0c88f8e7f0d8af   \n3  7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9   \n4  07588dd5d0252c7abc99b3834a81bf23741ead4b   \n\n                                               title   authorId  \\\n0  Detecting linguistic idiosyncratic interests i...    3188285   \n1  Bigrams and BiLSTMs Two Neural Networks for Se...    2782720   \n2  In Factuality: Efficient Integration of Releva...  144748442   \n3  Variational Graph Autoencoding as Cheap Superv...   46331602   \n4  LIMIT-BERT : Linguistics Informed Multi-Task BERT   30887404   \n\n          authorName                                           abstract  year  \\\n0  Masoud Rouhizadeh  Children with autism spectrum disorder often e...  2014   \n1       Yuri Bizzoni  We present and compare two alternative deep ne...  2018   \n2      Peter Vickers  Visual Question Answering (VQA) methods aim at...  2021   \n3           Irene Li  Coreference resolution over semantic graphs li...  2022   \n4         Junru Zhou  In this paper, we present Linguistics Informed...  2019   \n\n                venue                                             labels  \n0         CLPsych@ACL  Children with autism spectrum disorder often e...  \n1  Fig-Lang@NAACL-HLT  We present and compare two alternative deep ne...  \n2                 ACL  Visual Question Answering (VQA) methods aim at...  \n3                 ACL  Coreference resolution over semantic graphs li...  \n4            FINDINGS  In this paper, we present Linguistics Informed...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paperId</th>\n      <th>title</th>\n      <th>authorId</th>\n      <th>authorName</th>\n      <th>abstract</th>\n      <th>year</th>\n      <th>venue</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n      <td>Detecting linguistic idiosyncratic interests i...</td>\n      <td>3188285</td>\n      <td>Masoud Rouhizadeh</td>\n      <td>Children with autism spectrum disorder often e...</td>\n      <td>2014</td>\n      <td>CLPsych@ACL</td>\n      <td>Children with autism spectrum disorder often e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n      <td>Bigrams and BiLSTMs Two Neural Networks for Se...</td>\n      <td>2782720</td>\n      <td>Yuri Bizzoni</td>\n      <td>We present and compare two alternative deep ne...</td>\n      <td>2018</td>\n      <td>Fig-Lang@NAACL-HLT</td>\n      <td>We present and compare two alternative deep ne...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n      <td>In Factuality: Efficient Integration of Releva...</td>\n      <td>144748442</td>\n      <td>Peter Vickers</td>\n      <td>Visual Question Answering (VQA) methods aim at...</td>\n      <td>2021</td>\n      <td>ACL</td>\n      <td>Visual Question Answering (VQA) methods aim at...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9</td>\n      <td>Variational Graph Autoencoding as Cheap Superv...</td>\n      <td>46331602</td>\n      <td>Irene Li</td>\n      <td>Coreference resolution over semantic graphs li...</td>\n      <td>2022</td>\n      <td>ACL</td>\n      <td>Coreference resolution over semantic graphs li...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>07588dd5d0252c7abc99b3834a81bf23741ead4b</td>\n      <td>LIMIT-BERT : Linguistics Informed Multi-Task BERT</td>\n      <td>30887404</td>\n      <td>Junru Zhou</td>\n      <td>In this paper, we present Linguistics Informed...</td>\n      <td>2019</td>\n      <td>FINDINGS</td>\n      <td>In this paper, we present Linguistics Informed...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing training/validation data\n",
    "# It combines the columns that are given in the original dataset, to have only one independent variable column\n",
    "data, merged_data = createDataFrame(data, ['paperId'])\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "data": {
      "text/plain": "    authorId         authorName  \\\n0    3188285  Masoud Rouhizadeh   \n1    2782720       Yuri Bizzoni   \n2  144748442      Peter Vickers   \n3   46331602           Irene Li   \n4   30887404         Junru Zhou   \n\n                                              labels  \n0  children,with,autism,spectrum,disorder,often,e...  \n1  we,present,and,compare,two,alternative,deep,ne...  \n2  visual,question,answering,vqa,methods,aim,at,l...  \n3  coreference,resolution,over,semantic,graphs,li...  \n4  in,this,paper,we,present,linguistics,informed,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>authorId</th>\n      <th>authorName</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3188285</td>\n      <td>Masoud Rouhizadeh</td>\n      <td>children,with,autism,spectrum,disorder,often,e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2782720</td>\n      <td>Yuri Bizzoni</td>\n      <td>we,present,and,compare,two,alternative,deep,ne...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>144748442</td>\n      <td>Peter Vickers</td>\n      <td>visual,question,answering,vqa,methods,aim,at,l...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>46331602</td>\n      <td>Irene Li</td>\n      <td>coreference,resolution,over,semantic,graphs,li...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30887404</td>\n      <td>Junru Zhou</td>\n      <td>in,this,paper,we,present,linguistics,informed,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "# Pre-processing test data (same process as above, but for test dataset)\n",
    "test, merged_test = createDataFrame(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "# Replace stopwords, symbols\n",
    "# cleans out all the stopwords (the, a, an, etc.)\n",
    "# removes all symbols and numbers\n",
    "\n",
    "replace = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "replace_symbols = re.compile('[^0-9a-z #+_]')\n",
    "replace_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_labels(labels):\n",
    "    labels = labels.lower()  # lowercase labels\n",
    "    labels = replace.sub(' ', labels)  # replace REPLACE_BY_SPACE_RE symbols by space in labels\n",
    "    labels = replace_symbols.sub('', labels)  # delete symbols which are in BAD_SYMBOLS_RE from labels\n",
    "    labels = ' '.join(word for word in labels.split() if word not in replace_stopwords)  # delete stopwords from labels\n",
    "    return labels\n",
    "\n",
    "merged_data['labels'] = merged_data['labels'].apply(clean_labels)  # applies the above loop to the data\n",
    "merged_test['labels'] = merged_test['labels'].apply(clean_labels)  # applies the loop to test data\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()  # seperates the sentences to get single words\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()  # lemmatizes text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "merged_data['labels'] = merged_data['labels'].apply(lemmatize_text)\n",
    "merged_data['labels'] = [','.join(map(str, l)) for l in merged_data['labels']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9d5a645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects only the authors that are appearing in the dataset at least 4 times\n",
    "merged_data = merged_data.groupby('authorId').filter(lambda x: x['authorId'].shape[0] >= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8d7140f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1        present,compare,two,alternative,deep,neural,ar...\n6        describe,grammarless,method,simultaneously,bra...\n8        paper,describes,limsi,participation,wmt,shared...\n10       little,work,modeling,morphological,well,formed...\n13       transfer,learning,pre,trained,neural,language,...\n                               ...                        \n12104    multilingual,representation,embed,word,many,la...\n12112    count,based,distributional,semantic,model,suff...\n12116    study,event,detection,problem,new,type,extensi...\n12119    transformer,widely,used,state,art,machine,tran...\n12120    pre,trained,word,embeddings,improve,performanc...\nName: labels, Length: 3268, dtype: object"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data['labels']  # preview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "# Determine independent and dependent variables\n",
    "X = merged_data['labels']\n",
    "y = merged_data['authorId']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "data": {
      "text/plain": "1     present,compare,two,alternative,deep,neural,ar...\n6     describe,grammarless,method,simultaneously,bra...\n8     paper,describes,limsi,participation,wmt,shared...\n10    little,work,modeling,morphological,well,formed...\n13    transfer,learning,pre,trained,neural,language,...\nName: labels, dtype: object"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "data": {
      "text/plain": "<2777x143803 sparse matrix of type '<class 'numpy.float64'>'\n\twith 349581 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "tfidf_transformer = TfidfTransformer()  # transforms the hashed vectorized texted by using tfidf transformer\n",
    "\n",
    "X_train_hashed = vectorizer.fit_transform(X_train)\n",
    "X_test_hashed = vectorizer.transform(X_test)\n",
    "\n",
    "X_train = tfidf_transformer.fit_transform(X_train_hashed)\n",
    "X_test = tfidf_transformer.transform(X_test_hashed)\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with  50  batches and  5  epochs\n",
      "Accuracy: 0.2505091649694501\n",
      "Testing with  50  batches and  10  epochs\n",
      "Accuracy: 0.26272912423625255\n",
      "Testing with  50  batches and  25  epochs\n",
      "Accuracy: 0.2545824847250509\n",
      "Testing with  50  batches and  50  epochs\n",
      "Accuracy: 0.2525458248472505\n",
      "Testing with  100  batches and  5  epochs\n",
      "Accuracy: 0.2525458248472505\n",
      "Testing with  100  batches and  10  epochs\n",
      "Accuracy: 0.25865580448065173\n",
      "Testing with  100  batches and  25  epochs\n",
      "Accuracy: 0.25661914460285135\n",
      "Testing with  100  batches and  50  epochs\n",
      "Accuracy: 0.2545824847250509\n",
      "Testing with  250  batches and  5  epochs\n",
      "Accuracy: 0.2606924643584521\n",
      "Testing with  250  batches and  10  epochs\n",
      "Accuracy: 0.26476578411405294\n",
      "Testing with  250  batches and  25  epochs\n",
      "Accuracy: 0.2545824847250509\n",
      "Testing with  250  batches and  50  epochs\n",
      "Accuracy: 0.2525458248472505\n",
      "Testing with  500  batches and  5  epochs\n",
      "Accuracy: 0.2525458248472505\n",
      "Testing with  500  batches and  10  epochs\n",
      "Accuracy: 0.2545824847250509\n",
      "Testing with  500  batches and  25  epochs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [187], line 33\u001B[0m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetermined best: batches=\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_batch, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, epochs=\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_epoch)\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m create_model(best_batch, best_epoch)\n\u001B[1;32m---> 33\u001B[0m best \u001B[38;5;241m=\u001B[39m \u001B[43mbest_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy on testing data :\u001B[39m\u001B[38;5;124m\"\u001B[39m, best\u001B[38;5;241m.\u001B[39mscore(X_test, Y_test))\n",
      "Cell \u001B[1;32mIn [187], line 20\u001B[0m, in \u001B[0;36mbest_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n_epoch \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m25\u001B[39m, \u001B[38;5;241m50\u001B[39m]:\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTesting with \u001B[39m\u001B[38;5;124m\"\u001B[39m, n_batches, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m batches and \u001B[39m\u001B[38;5;124m\"\u001B[39m, n_epoch, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 20\u001B[0m     attempt \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m attempt\u001B[38;5;241m.\u001B[39mscore(X_test, Y_test)\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m accuracy \u001B[38;5;241m>\u001B[39m best_acc:\n",
      "Cell \u001B[1;32mIn [187], line 7\u001B[0m, in \u001B[0;36mcreate_model\u001B[1;34m(batchsize, epoch)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(batches):\n\u001B[1;32m----> 7\u001B[0m         \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatchsize\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatchsize\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatchsize\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatchsize\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:848\u001B[0m, in \u001B[0;36mBaseSGDClassifier.partial_fit\u001B[1;34m(self, X, y, classes, sample_weight)\u001B[0m\n\u001B[0;32m    836\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_weight \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    837\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    838\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_weight \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not supported for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    839\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartial_fit. In order to use \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m weights,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    846\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_weight)\n\u001B[0;32m    847\u001B[0m     )\n\u001B[1;32m--> 848\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_partial_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    851\u001B[0m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mC\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:619\u001B[0m, in \u001B[0;36mBaseSGDClassifier._partial_fit\u001B[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001B[0m\n\u001B[0;32m    617\u001B[0m \u001B[38;5;66;03m# delegate to concrete training procedure\u001B[39;00m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_classes \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m--> 619\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_multiclass\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    620\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    621\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[43m        \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    623\u001B[0m \u001B[43m        \u001B[49m\u001B[43mC\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    624\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    626\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m n_classes \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_binary(\n\u001B[0;32m    630\u001B[0m         X,\n\u001B[0;32m    631\u001B[0m         y,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    636\u001B[0m         max_iter\u001B[38;5;241m=\u001B[39mmax_iter,\n\u001B[0;32m    637\u001B[0m     )\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:763\u001B[0m, in \u001B[0;36mBaseSGDClassifier._fit_multiclass\u001B[1;34m(self, X, y, alpha, C, learning_rate, sample_weight, max_iter)\u001B[0m\n\u001B[0;32m    761\u001B[0m random_state \u001B[38;5;241m=\u001B[39m check_random_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state)\n\u001B[0;32m    762\u001B[0m seeds \u001B[38;5;241m=\u001B[39m random_state\u001B[38;5;241m.\u001B[39mrandint(MAX_INT, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_))\n\u001B[1;32m--> 763\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    764\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequire\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msharedmem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m    765\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    766\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfit_binary\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    767\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[43m        \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    772\u001B[0m \u001B[43m        \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    773\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    774\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    775\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_expanded_class_weight\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    776\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    777\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    778\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    779\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    780\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    781\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mseeds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[38;5;66;03m# take the maximum of n_iter_ over every binary fit\u001B[39;00m\n\u001B[0;32m    785\u001B[0m n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1085\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1088\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1089\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1092\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 901\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    902\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    818\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 819\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    820\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    822\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    823\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 597\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 117\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:456\u001B[0m, in \u001B[0;36mfit_binary\u001B[1;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001B[0m\n\u001B[0;32m    452\u001B[0m seed \u001B[38;5;241m=\u001B[39m random_state\u001B[38;5;241m.\u001B[39mrandint(MAX_INT)\n\u001B[0;32m    454\u001B[0m tol \u001B[38;5;241m=\u001B[39m est\u001B[38;5;241m.\u001B[39mtol \u001B[38;5;28;01mif\u001B[39;00m est\u001B[38;5;241m.\u001B[39mtol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf\n\u001B[1;32m--> 456\u001B[0m coef, intercept, average_coef, average_intercept, n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[43m_plain_sgd\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43maverage_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m    \u001B[49m\u001B[43maverage_intercept\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_function_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpenalty_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m    \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ml1_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mearly_stopping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_score_cb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_iter_no_change\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_intercept\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    475\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpos_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mneg_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meta0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpower_t\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintercept_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    486\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m est\u001B[38;5;241m.\u001B[39maverage:\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(est\u001B[38;5;241m.\u001B[39mclasses_) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def create_model(batchsize, epoch):\n",
    "    model = SGDClassifier()  # The classifier to use, this is supposed to be best applicable to large datasets\n",
    "    batches = int(X_train.shape[0] / batchsize) + 1\n",
    "    samples = X_train.shape[0]\n",
    "    for i in range(epoch):\n",
    "        for j in range(batches):\n",
    "            model.partial_fit(X_train[j * batchsize:min(samples, (j + 1) * batchsize)],\n",
    "                              Y_train[j * batchsize:min(samples, (j + 1) * batchsize)],\n",
    "                              classes=np.unique(y))\n",
    "    return model\n",
    "\n",
    "def best_model():\n",
    "    best_acc = -1\n",
    "    best_epoch = -1\n",
    "    best_batch = -1\n",
    "\n",
    "    for n_batches in [50, 100, 250, 500, 1000]:\n",
    "        for n_epoch in [5, 10, 25, 50]:\n",
    "            print(\"Testing with \", n_batches, \" batches and \", n_epoch, \" epochs\")\n",
    "            attempt = create_model(n_batches, n_epoch)\n",
    "\n",
    "            accuracy = attempt.score(X_test, Y_test)\n",
    "            if accuracy > best_acc:\n",
    "                best_acc = accuracy\n",
    "                best_epoch = n_epoch\n",
    "                best_batch = n_batches\n",
    "\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    print(\"Determined best: batches=\", best_batch, \", epochs=\", best_epoch)\n",
    "    return create_model(best_batch, best_epoch)\n",
    "\n",
    "best = best_model()\n",
    "print(\"Accuracy on testing data :\", best.score(X_test, Y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f17b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "# https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py\n",
    "# https://medium.com/mlearning-ai/out-of-core-multi-label-text-classification-with-scikit-learn-14afa4c1bb75\n",
    "# https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e735b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = merged_test['labels']  # the test data labels that we will apply to transform to numerical\n",
    "X_new = vectorizer.transform(new_data)  # transforms the data by using the vectorizer\n",
    "y_pred = best.predict(X_new)  # predicts the new values of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test['prediction'] = y_pred.tolist()  # puts the predicted data to list\n",
    "final = merged_test.set_axis(['paperId', 'labels', 'authorId'], axis=1, inplace=False)  # changes the axis labels\n",
    "final = final.drop(labels=['labels'], axis=1)  # drops the labels column to get final result of only paperId & authorId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d426a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
