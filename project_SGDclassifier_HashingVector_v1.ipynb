{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6630da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jasoncharnock/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d4f670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/vwzjd2dn1ndg85vgr9jy1xv80000gn/T/ipykernel_23772/2075360187.py:18: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  merged_data['labels'] = merged_data['labels'].str.replace('[^A-Za-z]',' ')\n",
      "/var/folders/c4/vwzjd2dn1ndg85vgr9jy1xv80000gn/T/ipykernel_23772/2075360187.py:33: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  merged_test['labels'] = merged_test['labels'].str.replace('[^A-Za-z]',' ')\n"
     ]
    }
   ],
   "source": [
    "# Prediction model using CountVectorizer() and LogisticRegression(), which is insufficient to train all data from training set \n",
    "\n",
    "# Opening JSON files\n",
    "\n",
    "f = open('train.json',)\n",
    "data = json.load(f)\n",
    "\n",
    "g = open('test.json',)\n",
    "test = json.load(g)\n",
    "\n",
    "# Pre-processing training/validation data \n",
    "    # It combines the columns that are given in the original dataset, to have only one independent variable column\n",
    "data = pd.DataFrame(data)\n",
    "data['year'] = data['year'].astype(str)\n",
    "data['labels']=data[['title', 'abstract', 'year', 'venue']].apply(lambda x:','.join(x), axis = 1) \n",
    "merged_data=data.drop(labels =['title', 'abstract', 'year', 'venue', 'paperId'], axis = 1)\n",
    "merged_data.head()\n",
    "\n",
    "# This replaces capital letters, and some symbols. It lowers all the texts, strips and splits and converts it to strings\n",
    "merged_data['labels'] = merged_data['labels'].str.replace('[^A-Za-z]',' ')\n",
    "merged_data['labels'] = merged_data['labels'].str.lower()\n",
    "merged_data[\"labels\"] = merged_data[\"labels\"].str.strip()\n",
    "merged_data[\"labels\"] = merged_data[\"labels\"].str.split() \n",
    "merged_data['labels'] = [','.join(map(str, l)) for l in merged_data['labels']]\n",
    "\n",
    "\n",
    "# Pre-processing test data (same process as above, but for test dataset)\n",
    "\n",
    "test = pd.DataFrame(test)\n",
    "test['year'] = test['year'].astype(str)\n",
    "test['labels']=test[['title', 'abstract', 'year', 'venue']].apply(lambda x:','.join(x), axis = 1) \n",
    "merged_test=test.drop(labels =['title', 'abstract', 'year', 'venue'], axis = 1)\n",
    "merged_test.head()\n",
    "\n",
    "merged_test['labels'] = merged_test['labels'].str.replace('[^A-Za-z]',' ')\n",
    "merged_test['labels'] = merged_test['labels'].str.lower()\n",
    "merged_test[\"labels\"] = merged_test[\"labels\"].str.strip()\n",
    "merged_test[\"labels\"] = merged_test[\"labels\"].str.split() \n",
    "merged_test['labels'] = [','.join(map(str, l)) for l in merged_test['labels']]\n",
    "\n",
    "# Replace stopwords, symbols\n",
    "    # cleans out all the stopwords (the, a, an, etc.)\n",
    "    # removes all symbols and numbers\n",
    "\n",
    "replace = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "replace_symbols = re.compile('[^0-9a-z #+_]')\n",
    "replace_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_labels(labels):\n",
    "    \n",
    "    labels = labels.lower() # lowercase labels\n",
    "    labels = replace.sub(' ', labels) # replace REPLACE_BY_SPACE_RE symbols by space in labels\n",
    "    labels = replace_symbols.sub('', labels) # delete symbols which are in BAD_SYMBOLS_RE from labels\n",
    "    labels = ' '.join(word for word in labels.split() if word not in replace_stopwords) # delete stopwors from labels\n",
    "    return labels\n",
    "    \n",
    "merged_data['labels'] = merged_data['labels'].apply(clean_labels) # applies the above loop to the data\n",
    "    \n",
    "merged_test['labels'] = merged_test['labels'].apply(clean_labels) # applies the loop to test data\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer() # seperates the sentences to get single words\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer() # lemmatizes text\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "merged_data['labels'] = merged_data['labels'].apply(lemmatize_text)\n",
    "\n",
    "merged_data['labels'] = [','.join(map(str, l)) for l in merged_data['labels']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects only the authors that are appearing in the dataset at least 4 times\n",
    "merged_data = merged_data.groupby('authorId').filter(lambda x : x['authorId'].shape[0]>=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7140f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        bigram,bilstms,two,neural,network,sequential,m...\n",
       "6        algorithm,simultaneously,bracketing,parallel,t...\n",
       "8        limsi,submission,wmt,qe,task,paper,describes,l...\n",
       "10       graph,auto,encoder,model,derivational,morpholo...\n",
       "13       transfer,learning,impact,linguistic,knowledge,...\n",
       "                               ...                        \n",
       "12104    gender,bias,multilingual,embeddings,cross,ling...\n",
       "12112    improving,semantic,composition,offset,inferenc...\n",
       "12116    two,stage,approach,extending,event,detection,n...\n",
       "12119    parameter,born,equal,attention,mostly,need,tra...\n",
       "12120    reconstruction,word,embeddings,sub,word,parame...\n",
       "Name: labels, Length: 3268, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data['labels'] # preview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc0188da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2777x262144 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 401794 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine independent and dependent variables \n",
    "\n",
    "X = merged_data['labels']\n",
    "y = merged_data['authorId']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "vectorizer = HashingVectorizer(ngram_range=(1,2), n_features=2**18) \n",
    "# applies the hashing vectorizer (found to be the best for large text datasets)\n",
    "X_train_hashed = vectorizer.transform(X_train)\n",
    "tfidf_transformer=TfidfTransformer() # transforms the hashed vectorized texted by using tfidf transformer\n",
    "# TFIDF works by proportionally increasing the number of times a word appears in the document \n",
    "# but is counterbalanced by the number of documents in which it is present\n",
    "# https://www.analyticsvidhya.com/blog/2021/07/bag-of-words-vs-tfidf-vectorization-a-hands-on-tutorial/\n",
    "X_train = tfidf_transformer.fit_transform(X_train_hashed)\n",
    "X_test_hashed = vectorizer.transform(X_test)\n",
    "X_test = tfidf_transformer.transform(X_test_hashed)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9b21384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in j... 0 0 ----2is: 2777 500\n",
      "in j... 1 500 ----2is: 2777 1000\n",
      "in j... 2 1000 ----2is: 2777 1500\n",
      "in j... 3 1500 ----2is: 2777 2000\n",
      "in j... 4 2000 ----2is: 2777 2500\n",
      "in j... 5 2500 ----2is: 2777 3000\n",
      "in j... 0 0 ----2is: 2777 500\n",
      "in j... 1 500 ----2is: 2777 1000\n",
      "in j... 2 1000 ----2is: 2777 1500\n",
      "in j... 3 1500 ----2is: 2777 2000\n",
      "in j... 4 2000 ----2is: 2777 2500\n",
      "in j... 5 2500 ----2is: 2777 3000\n",
      "in j... 0 0 ----2is: 2777 500\n",
      "in j... 1 500 ----2is: 2777 1000\n",
      "in j... 2 1000 ----2is: 2777 1500\n",
      "in j... 3 1500 ----2is: 2777 2000\n",
      "in j... 4 2000 ----2is: 2777 2500\n",
      "in j... 5 2500 ----2is: 2777 3000\n",
      "in j... 0 0 ----2is: 2777 500\n",
      "in j... 1 500 ----2is: 2777 1000\n",
      "in j... 2 1000 ----2is: 2777 1500\n",
      "in j... 3 1500 ----2is: 2777 2000\n",
      "in j... 4 2000 ----2is: 2777 2500\n",
      "in j... 5 2500 ----2is: 2777 3000\n",
      "in j... 0 0 ----2is: 2777 500\n",
      "in j... 1 500 ----2is: 2777 1000\n",
      "in j... 2 1000 ----2is: 2777 1500\n",
      "in j... 3 1500 ----2is: 2777 2000\n",
      "in j... 4 2000 ----2is: 2777 2500\n",
      "in j... 5 2500 ----2is: 2777 3000\n",
      "Accuracy on testing data : 0.26476578411405294\n"
     ]
    }
   ],
   "source": [
    "# This trains the model\n",
    "\n",
    "epoch = 5 # the amount of times that it will run\n",
    "batchsize = 500 # the amount of data you put in per batch\n",
    "model = SGDClassifier() # The classifier to use, this is supposed to be best applicable to large datasets\n",
    "batches = int(X_train.shape[0]/batchsize) + 1\n",
    "samples = X_train.shape[0]\n",
    "for i in range(epoch):\n",
    "    for j in range(batches):\n",
    "        print('in j...', j, j*batchsize, '----2is:',samples, (j+1)*batchsize )\n",
    "        model.partial_fit(X_train[j*batchsize:min(samples,(j+1)*batchsize)], \n",
    "                          Y_train[j*batchsize:min(samples,(j+1)*batchsize)], \n",
    "                          classes=np.unique(y))\n",
    "print (\"Accuracy on testing data :\", model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f17b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "# https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py\n",
    "# https://medium.com/mlearning-ai/out-of-core-multi-label-text-classification-with-scikit-learn-14afa4c1bb75\n",
    "# https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e735b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = merged_test['labels'] # the test data labels that we will apply to transform to numerical\n",
    "X_new = vectorizer.transform(new_data) # transforms the data by using the vectorizer\n",
    "y_pred = model.predict(X_new) # predicts the new values of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9121498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test['prediction'] = y_pred.tolist() # puts the predicted data to list\n",
    "final = merged_test.set_axis(['paperId', 'labels', 'authorId'], axis=1, inplace=False) # changes the axis labels\n",
    "final = final.drop(labels = ['labels'], axis = 1) # drops the labels column to get final result of only paperId & authorId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d426a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>authorId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86e1aaa0c47659e08a896e9889384eb1e5401e6a</td>\n",
       "      <td>143655216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8d3076c38f56df22052567f4783c670d8e860f09</td>\n",
       "      <td>14488816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7c400ee676d427eeda1aad5c1c54c316f0b9773d</td>\n",
       "      <td>1788050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185e7d2a761594451b02ace240356dadad2aef78</td>\n",
       "      <td>3115181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80</td>\n",
       "      <td>1989344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>069ebed0ba7adec30faa5c5e008053cf3eefc589</td>\n",
       "      <td>1678591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6527</th>\n",
       "      <td>b6e9fdc3e7bc4d379ee733b07199fe2a8336dd94</td>\n",
       "      <td>47210642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528</th>\n",
       "      <td>5019da491732e412fafea4e1511818fd684cc1f1</td>\n",
       "      <td>1750769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529</th>\n",
       "      <td>eca16c1c776406abd0d966653a705f945bd4b520</td>\n",
       "      <td>1779225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6530</th>\n",
       "      <td>66b32bdb57d67bc10cb11605b5ba9bad8741789f</td>\n",
       "      <td>3084761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6531 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paperId   authorId\n",
       "0     86e1aaa0c47659e08a896e9889384eb1e5401e6a  143655216\n",
       "1     8d3076c38f56df22052567f4783c670d8e860f09   14488816\n",
       "2     7c400ee676d427eeda1aad5c1c54c316f0b9773d    1788050\n",
       "3     185e7d2a761594451b02ace240356dadad2aef78    3115181\n",
       "4     e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80    1989344\n",
       "...                                        ...        ...\n",
       "6526  069ebed0ba7adec30faa5c5e008053cf3eefc589    1678591\n",
       "6527  b6e9fdc3e7bc4d379ee733b07199fe2a8336dd94   47210642\n",
       "6528  5019da491732e412fafea4e1511818fd684cc1f1    1750769\n",
       "6529  eca16c1c776406abd0d966653a705f945bd4b520    1779225\n",
       "6530  66b32bdb57d67bc10cb11605b5ba9bad8741789f    3084761\n",
       "\n",
       "[6531 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
