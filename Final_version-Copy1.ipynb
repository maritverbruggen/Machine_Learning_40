{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61973fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasoncharnock/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jasoncharnock/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import linear_model\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from datetime import datetime\n",
    "from scipy import sparse\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94f6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON files\n",
    "\n",
    "f = open('train.json')\n",
    "data = json.load(f)\n",
    "\n",
    "g = open('test.json', )\n",
    "test = json.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1deb386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n",
       "      <td>Detecting linguistic idiosyncratic interests i...</td>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>Children with autism spectrum disorder often e...</td>\n",
       "      <td>2014</td>\n",
       "      <td>CLPsych@ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n",
       "      <td>Bigrams and BiLSTMs Two Neural Networks for Se...</td>\n",
       "      <td>2782720</td>\n",
       "      <td>Yuri Bizzoni</td>\n",
       "      <td>We present and compare two alternative deep ne...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fig-Lang@NAACL-HLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n",
       "      <td>In Factuality: Efficient Integration of Releva...</td>\n",
       "      <td>144748442</td>\n",
       "      <td>Peter Vickers</td>\n",
       "      <td>Visual Question Answering (VQA) methods aim at...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9</td>\n",
       "      <td>Variational Graph Autoencoding as Cheap Superv...</td>\n",
       "      <td>46331602</td>\n",
       "      <td>Irene Li</td>\n",
       "      <td>Coreference resolution over semantic graphs li...</td>\n",
       "      <td>2022</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07588dd5d0252c7abc99b3834a81bf23741ead4b</td>\n",
       "      <td>LIMIT-BERT : Linguistics Informed Multi-Task BERT</td>\n",
       "      <td>30887404</td>\n",
       "      <td>Junru Zhou</td>\n",
       "      <td>In this paper, we present Linguistics Informed...</td>\n",
       "      <td>2019</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId  \\\n",
       "0  0b341b6938308a6d5f47edf490f6e46eae3835fa   \n",
       "1  c682727ee058aadbe9dbf838dcb036322818f588   \n",
       "2  0f9b5b32229a7245e43754430c0c88f8e7f0d8af   \n",
       "3  7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9   \n",
       "4  07588dd5d0252c7abc99b3834a81bf23741ead4b   \n",
       "\n",
       "                                               title   authorId  \\\n",
       "0  Detecting linguistic idiosyncratic interests i...    3188285   \n",
       "1  Bigrams and BiLSTMs Two Neural Networks for Se...    2782720   \n",
       "2  In Factuality: Efficient Integration of Releva...  144748442   \n",
       "3  Variational Graph Autoencoding as Cheap Superv...   46331602   \n",
       "4  LIMIT-BERT : Linguistics Informed Multi-Task BERT   30887404   \n",
       "\n",
       "          authorName                                           abstract  year  \\\n",
       "0  Masoud Rouhizadeh  Children with autism spectrum disorder often e...  2014   \n",
       "1       Yuri Bizzoni  We present and compare two alternative deep ne...  2018   \n",
       "2      Peter Vickers  Visual Question Answering (VQA) methods aim at...  2021   \n",
       "3           Irene Li  Coreference resolution over semantic graphs li...  2022   \n",
       "4         Junru Zhou  In this paper, we present Linguistics Informed...  2019   \n",
       "\n",
       "                venue  \n",
       "0         CLPsych@ACL  \n",
       "1  Fig-Lang@NAACL-HLT  \n",
       "2                 ACL  \n",
       "3                 ACL  \n",
       "4            FINDINGS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data)\n",
    "test = pd.DataFrame(test)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0973c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "replace_symbols = re.compile('[^0-9a-z #+_]')\n",
    "replace_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_labels(labels):\n",
    "    labels = labels.lower()  # lowercase labels\n",
    "    labels = labels.replace('-', ' ')\n",
    "    labels = replace.sub(' ', labels)  # replace REPLACE_BY_SPACE_RE symbols by space in labels\n",
    "    labels = replace_symbols.sub('', labels)  # delete symbols which are in BAD_SYMBOLS_RE from labels\n",
    "    labels = ' '.join(word for word in labels.split() if word not in replace_stopwords)  # delete stopwords from labels\n",
    "    labels = labels.replace(' ', ',')\n",
    "    return labels\n",
    "\n",
    "data['title'] = data['title'].apply(clean_labels)\n",
    "data['abstract'] = data['abstract'].apply(clean_labels)\n",
    "\n",
    "test['title'] = test['title'].apply(clean_labels)\n",
    "test['abstract'] = test['abstract'].apply(clean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5095fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>b04ad4b86f46b0c6670f0da898a1300f385bf3d2</td>\n",
       "      <td>combining,recurrent,convolutional,neural,netwo...</td>\n",
       "      <td>4160376</td>\n",
       "      <td>Ngoc Thang Vu</td>\n",
       "      <td>paper,investigates,two,different,neural,archit...</td>\n",
       "      <td>2016</td>\n",
       "      <td>NAACL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paperId  \\\n",
       "4588  b04ad4b86f46b0c6670f0da898a1300f385bf3d2   \n",
       "\n",
       "                                                  title authorId  \\\n",
       "4588  combining,recurrent,convolutional,neural,netwo...  4160376   \n",
       "\n",
       "         authorName                                           abstract  year  \\\n",
       "4588  Ngoc Thang Vu  paper,investigates,two,different,neural,archit...  2016   \n",
       "\n",
       "      venue  \n",
       "4588  NAACL  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)\n",
    "data.loc[data['authorId'] == '4160376']\n",
    "\n",
    "# Use text blob to filter out words that are not in dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34ca3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()  # seperates the sentences to get single words\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()  # lemmatizes text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "data['title'] = data['title'].apply(lemmatize_text)\n",
    "data['title'] = [','.join(map(str, l)) for l in data['title']]\n",
    "data['abstract'] = data['abstract'].apply(lemmatize_text)\n",
    "data['abstract'] = [','.join(map(str, l)) for l in data['abstract']]\n",
    "\n",
    "test['title'] = test['title'].apply(lemmatize_text)\n",
    "test['title'] = [','.join(map(str, l)) for l in test['title']]\n",
    "test['abstract'] = test['abstract'].apply(lemmatize_text)\n",
    "test['abstract'] = [','.join(map(str, l)) for l in test['abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6025c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>b04ad4b86f46b0c6670f0da898a1300f385bf3d2</td>\n",
       "      <td>combining,recurrent,convolutional,neural,netwo...</td>\n",
       "      <td>4160376</td>\n",
       "      <td>Ngoc Thang Vu</td>\n",
       "      <td>paper,investigates,two,different,neural,archit...</td>\n",
       "      <td>2016</td>\n",
       "      <td>NAACL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paperId  \\\n",
       "4588  b04ad4b86f46b0c6670f0da898a1300f385bf3d2   \n",
       "\n",
       "                                                  title authorId  \\\n",
       "4588  combining,recurrent,convolutional,neural,netwo...  4160376   \n",
       "\n",
       "         authorName                                           abstract  year  \\\n",
       "4588  Ngoc Thang Vu  paper,investigates,two,different,neural,archit...  2016   \n",
       "\n",
       "      venue  \n",
       "4588  NAACL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.head(20)\n",
    "data.loc[data['authorId'] == '4160376']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13fd19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86e1aaa0c47659e08a896e9889384eb1e5401e6a</td>\n",
       "      <td>exploring,linear,subspace,hypothesis,gender,bi...</td>\n",
       "      <td>bolukbasi,et,al,2016,presents,one,first,gender...</td>\n",
       "      <td>2020</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8d3076c38f56df22052567f4783c670d8e860f09</td>\n",
       "      <td>hyknow,end,end,task,oriented,dialog,modeling,h...</td>\n",
       "      <td>task,oriented,dialog,tod,systems,typically,man...</td>\n",
       "      <td>2021</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7c400ee676d427eeda1aad5c1c54c316f0b9773d</td>\n",
       "      <td>multilingual,information,extraction,pipeline,i...</td>\n",
       "      <td>introduce,advanced,information,extraction,pipe...</td>\n",
       "      <td>2018</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185e7d2a761594451b02ace240356dadad2aef78</td>\n",
       "      <td>dice,loss,data,imbalanced,nlp,tasks</td>\n",
       "      <td>many,nlp,tasks,tagging,machine,reading,compreh...</td>\n",
       "      <td>2019</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80</td>\n",
       "      <td>attention,guided,graph,convolutional,networks,...</td>\n",
       "      <td>dependency,trees,convey,rich,structural,inform...</td>\n",
       "      <td>2019</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ca20f8146adf21294f048121f234c449299ef67e</td>\n",
       "      <td>combining,multiple,knowledge,sources,discourse...</td>\n",
       "      <td>predict,discourse,segment,boundaries,linguisti...</td>\n",
       "      <td>1995</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cbdb0d682a59933fb144124f1bfaec0ee3f3b04c</td>\n",
       "      <td>unsupervised,opinion,summarization,noising,den...</td>\n",
       "      <td>supervised,training,high,capacity,models,large...</td>\n",
       "      <td>2020</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>477cc0e5533ed08768f83d41718feaf7cbcaf3a6</td>\n",
       "      <td>towards,understanding,geometry,knowledge,graph...</td>\n",
       "      <td>knowledge,graph,kg,embedding,emerged,active,ar...</td>\n",
       "      <td>2018</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>53d8b356551a2361020a948f64454a6d599af69f</td>\n",
       "      <td>prefix,tuning,optimizing,continuous,prompts,ge...</td>\n",
       "      <td>fine,tuning,de,facto,way,leveraging,large,pret...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b9608e053896874d4f7d62f744057cf7105c5c90</td>\n",
       "      <td>data,driven,enough,revisiting,interactive,inst...</td>\n",
       "      <td>modeling,traditional,nlg,tasks,data,driven,tec...</td>\n",
       "      <td>2018</td>\n",
       "      <td>HRI 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>66901b630b2d0b248fb02bf6d7a6a9d6244f9d0a</td>\n",
       "      <td>sigtyp,2021,shared,task,robust,spoken,language...</td>\n",
       "      <td>language,identification,fundamental,speech,lan...</td>\n",
       "      <td>2021</td>\n",
       "      <td>SIGTYP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>82f9637e263251b2387c8e0c87b942bd1b6c3bdd</td>\n",
       "      <td>lexically,constrained,decoding,sequence,genera...</td>\n",
       "      <td>present,grid,beam,search,gbs,algorithm,extends...</td>\n",
       "      <td>2017</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64749a0087485f9106b3e9a4337d2b19436f9178</td>\n",
       "      <td>neural,modeling,multi,predicate,interactions,j...</td>\n",
       "      <td>performance,japanese,predicate,argument,struct...</td>\n",
       "      <td>2017</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32bfea8f6dd4010de139e391bfd894606fdc1399</td>\n",
       "      <td>incorporating,risk,factor,embeddings,pre,train...</td>\n",
       "      <td>reducing,rates,early,hospital,readmission,reco...</td>\n",
       "      <td>2020</td>\n",
       "      <td>CLINICALNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7ea243eef1bfb4c3aee61bcc7ee5e816c24b36fb</td>\n",
       "      <td>towards,toxic,positivity,detection</td>\n",
       "      <td>past,years,growing,concern,around,toxic,positi...</td>\n",
       "      <td>2022</td>\n",
       "      <td>SOCIALNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>a6ceb0b9db6346539551ed1ffc198b3703d36a7f</td>\n",
       "      <td>new,approaches,parsing,conjunctions,using,prolog</td>\n",
       "      <td>conjunctions,particularly,difficult,parse,trad...</td>\n",
       "      <td>1985</td>\n",
       "      <td>IJCAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9db4e74bd70af5f4bfe9d51c138eee92e3b60642</td>\n",
       "      <td>sampling,filtering,neural,machine,translation,...</td>\n",
       "      <td>neural,machine,translation,distillation,steali...</td>\n",
       "      <td>2021</td>\n",
       "      <td>NAACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cb2e1b91e7eb03bcf9132754f737ab00a424c227</td>\n",
       "      <td>back,basics,monolingual,alignment,exploiting,w...</td>\n",
       "      <td>present,simple,easy,replicate,monolingual,alig...</td>\n",
       "      <td>2014</td>\n",
       "      <td>TACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>c8434b04acfe1d300b56c0d369092038ec899a2b</td>\n",
       "      <td>classifying,relations,ranking,convolutional,ne...</td>\n",
       "      <td>relation,classification,important,semantic,pro...</td>\n",
       "      <td>2015</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15493bbb5387d60e7c77cee34528acd3acae7b65</td>\n",
       "      <td>efficient,passage,retrieval,hashing,open,domai...</td>\n",
       "      <td>state,art,open,domain,question,answering,syste...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     paperId  \\\n",
       "0   86e1aaa0c47659e08a896e9889384eb1e5401e6a   \n",
       "1   8d3076c38f56df22052567f4783c670d8e860f09   \n",
       "2   7c400ee676d427eeda1aad5c1c54c316f0b9773d   \n",
       "3   185e7d2a761594451b02ace240356dadad2aef78   \n",
       "4   e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80   \n",
       "5   ca20f8146adf21294f048121f234c449299ef67e   \n",
       "6   cbdb0d682a59933fb144124f1bfaec0ee3f3b04c   \n",
       "7   477cc0e5533ed08768f83d41718feaf7cbcaf3a6   \n",
       "8   53d8b356551a2361020a948f64454a6d599af69f   \n",
       "9   b9608e053896874d4f7d62f744057cf7105c5c90   \n",
       "10  66901b630b2d0b248fb02bf6d7a6a9d6244f9d0a   \n",
       "11  82f9637e263251b2387c8e0c87b942bd1b6c3bdd   \n",
       "12  64749a0087485f9106b3e9a4337d2b19436f9178   \n",
       "13  32bfea8f6dd4010de139e391bfd894606fdc1399   \n",
       "14  7ea243eef1bfb4c3aee61bcc7ee5e816c24b36fb   \n",
       "15  a6ceb0b9db6346539551ed1ffc198b3703d36a7f   \n",
       "16  9db4e74bd70af5f4bfe9d51c138eee92e3b60642   \n",
       "17  cb2e1b91e7eb03bcf9132754f737ab00a424c227   \n",
       "18  c8434b04acfe1d300b56c0d369092038ec899a2b   \n",
       "19  15493bbb5387d60e7c77cee34528acd3acae7b65   \n",
       "\n",
       "                                                title  \\\n",
       "0   exploring,linear,subspace,hypothesis,gender,bi...   \n",
       "1   hyknow,end,end,task,oriented,dialog,modeling,h...   \n",
       "2   multilingual,information,extraction,pipeline,i...   \n",
       "3                 dice,loss,data,imbalanced,nlp,tasks   \n",
       "4   attention,guided,graph,convolutional,networks,...   \n",
       "5   combining,multiple,knowledge,sources,discourse...   \n",
       "6   unsupervised,opinion,summarization,noising,den...   \n",
       "7   towards,understanding,geometry,knowledge,graph...   \n",
       "8   prefix,tuning,optimizing,continuous,prompts,ge...   \n",
       "9   data,driven,enough,revisiting,interactive,inst...   \n",
       "10  sigtyp,2021,shared,task,robust,spoken,language...   \n",
       "11  lexically,constrained,decoding,sequence,genera...   \n",
       "12  neural,modeling,multi,predicate,interactions,j...   \n",
       "13  incorporating,risk,factor,embeddings,pre,train...   \n",
       "14                 towards,toxic,positivity,detection   \n",
       "15   new,approaches,parsing,conjunctions,using,prolog   \n",
       "16  sampling,filtering,neural,machine,translation,...   \n",
       "17  back,basics,monolingual,alignment,exploiting,w...   \n",
       "18  classifying,relations,ranking,convolutional,ne...   \n",
       "19  efficient,passage,retrieval,hashing,open,domai...   \n",
       "\n",
       "                                             abstract  year        venue  \n",
       "0   bolukbasi,et,al,2016,presents,one,first,gender...  2020        EMNLP  \n",
       "1   task,oriented,dialog,tod,systems,typically,man...  2021     FINDINGS  \n",
       "2   introduce,advanced,information,extraction,pipe...  2018        EMNLP  \n",
       "3   many,nlp,tasks,tagging,machine,reading,compreh...  2019          ACL  \n",
       "4   dependency,trees,convey,rich,structural,inform...  2019          ACL  \n",
       "5   predict,discourse,segment,boundaries,linguisti...  1995          ACL  \n",
       "6   supervised,training,high,capacity,models,large...  2020          ACL  \n",
       "7   knowledge,graph,kg,embedding,emerged,active,ar...  2018          ACL  \n",
       "8   fine,tuning,de,facto,way,leveraging,large,pret...  2021          ACL  \n",
       "9   modeling,traditional,nlg,tasks,data,driven,tec...  2018     HRI 2018  \n",
       "10  language,identification,fundamental,speech,lan...  2021       SIGTYP  \n",
       "11  present,grid,beam,search,gbs,algorithm,extends...  2017          ACL  \n",
       "12  performance,japanese,predicate,argument,struct...  2017          ACL  \n",
       "13  reducing,rates,early,hospital,readmission,reco...  2020  CLINICALNLP  \n",
       "14  past,years,growing,concern,around,toxic,positi...  2022    SOCIALNLP  \n",
       "15  conjunctions,particularly,difficult,parse,trad...  1985        IJCAI  \n",
       "16  neural,machine,translation,distillation,steali...  2021        NAACL  \n",
       "17  present,simple,easy,replicate,monolingual,alig...  2014         TACL  \n",
       "18  relation,classification,important,semantic,pro...  2015          ACL  \n",
       "19  state,art,open,domain,question,answering,syste...  2021          ACL  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf8529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.sample(1000, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86187893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1750769       13\n",
       "1747849       13\n",
       "51042088      12\n",
       "2854981       12\n",
       "3422953       11\n",
       "              ..\n",
       "40192974       1\n",
       "2013172        1\n",
       "2106294609     1\n",
       "5677323        1\n",
       "144928136      1\n",
       "Name: authorId, Length: 5625, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['authorId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c34eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3188285\n",
       "1        2782720\n",
       "2      144748442\n",
       "3       46331602\n",
       "4       30887404\n",
       "5       46649145\n",
       "6        2390150\n",
       "7      151474408\n",
       "8        1696542\n",
       "9      144518416\n",
       "10    1667898858\n",
       "11    2072874946\n",
       "12    1721683964\n",
       "13     145938140\n",
       "14      31333199\n",
       "15     145482266\n",
       "16      46177105\n",
       "17       1768065\n",
       "18      25062613\n",
       "19       1736049\n",
       "Name: authorId, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['authorId'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f831a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = data['title']\n",
    "abstract = data['abstract']\n",
    "year = data['year']\n",
    "venue = data['venue']\n",
    "author_id = data['authorId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3898d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_test = test['title']\n",
    "abstract_test = test['abstract']\n",
    "year_test = test['year']\n",
    "venue_test = test['venue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2f105e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(ngram_range=(1,2), n_features=2**18) # applies the hashing vectorizer (found to be the best for large text datasets)\n",
    "transformer = TfidfTransformer()\n",
    "encoder = LabelEncoder()\n",
    "count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "212f252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed title shape:  (12129, 262144)\n",
      "Transformed abstract shape:  (12129, 262144)\n",
      "Transformed venue shape:  (12129, 262144)\n",
      "Transformed year shape:  (12129,)\n",
      "[35 39 42 ... 42 39 35]\n"
     ]
    }
   ],
   "source": [
    "title_vect = vectorizer.transform(title)\n",
    "title = transformer.fit_transform(title_vect)\n",
    "print(\"Transformed title shape: \", title.shape)\n",
    "abstract_vect = vectorizer.transform(abstract)\n",
    "abstract = transformer.fit_transform(abstract_vect)\n",
    "print(\"Transformed abstract shape: \", abstract.shape)\n",
    "venue_vect = vectorizer.transform(venue)\n",
    "venue = transformer.fit_transform(venue_vect)\n",
    "print(\"Transformed venue shape: \", venue.shape)\n",
    "year = encoder.fit_transform(year)\n",
    "print(\"Transformed year shape: \", year.shape)\n",
    "print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29310621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed title shape:  (6531, 262144)\n",
      "Transformed abstract shape:  (6531, 262144)\n",
      "Transformed venue shape:  (6531, 262144)\n",
      "Transformed year shape:  (6531,)\n",
      "[41 42 39 ... 39  0 40]\n"
     ]
    }
   ],
   "source": [
    "title_vect = vectorizer.transform(title_test)\n",
    "title_test = transformer.transform(title_vect)\n",
    "print(\"Transformed title shape: \", title_test.shape)\n",
    "abstract_vect = vectorizer.transform(abstract_test)\n",
    "abstract_test = transformer.transform(abstract_vect)\n",
    "print(\"Transformed abstract shape: \", abstract_test.shape)\n",
    "venue_vect = vectorizer.transform(venue_test)\n",
    "venue_test = transformer.transform(venue_vect)\n",
    "print(\"Transformed venue shape: \", venue_test.shape)\n",
    "year_test = encoder.fit_transform(year_test)\n",
    "print(\"Transformed year shape: \", year_test.shape)\n",
    "print(year_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd377f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author id array shape:  (12129, 1)\n",
      "Year array shape:  (12129, 1)\n"
     ]
    }
   ],
   "source": [
    "# Converting year to a numpy array so that it can be stacked to the label matrix\n",
    "\n",
    "author_id = np.array(author_id)\n",
    "author_id = np.reshape(author_id, (12129, 1))\n",
    "\n",
    "print(\"Author id array shape: \", author_id.shape)\n",
    "\n",
    "year = np.array(year)\n",
    "year = np.reshape(year, (12129, 1))\n",
    "\n",
    "print(\"Year array shape: \", year.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5b88ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year array shape:  (6531, 1)\n"
     ]
    }
   ],
   "source": [
    "# Converting year to a numpy array so that it can be stacked to the label matrix\n",
    "\n",
    "year_test = np.array(year_test)\n",
    "year_test = np.reshape(year_test, (6531, 1))\n",
    "\n",
    "print(\"Year array shape: \", year_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac3882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 258795)\t0.035023989585094055\n",
      "  (0, 257647)\t0.08848060101451363\n",
      "  (0, 255850)\t-0.060536479459789745\n",
      "  (0, 255817)\t-0.07618787982815582\n",
      "  (0, 255613)\t-0.0471491508805175\n",
      "  (0, 254172)\t0.09528209917977884\n",
      "  (0, 252630)\t0.09528209917977884\n",
      "  (0, 251637)\t-0.035500102219976244\n",
      "  (0, 250310)\t-0.06535445086022985\n",
      "  (0, 249666)\t0.06495128320749166\n",
      "  (0, 247006)\t-0.04006113636975368\n",
      "  (0, 242543)\t0.07770048147384456\n",
      "  (0, 239330)\t-0.06870938996185565\n",
      "  (0, 237400)\t0.02895130010528154\n",
      "  (0, 231177)\t-0.0696921031988147\n",
      "  (0, 230022)\t-0.049662008349155976\n",
      "  (0, 229945)\t-0.03855862468657214\n",
      "  (0, 228395)\t-0.08629100766778991\n",
      "  (0, 225536)\t0.05596019169550275\n",
      "  (0, 225288)\t-0.08450197963910976\n",
      "  (0, 220555)\t-0.060395290150901726\n",
      "  (0, 219198)\t-0.08450197963910976\n",
      "  (0, 218769)\t0.08629100766778991\n",
      "  (0, 218287)\t-0.043267924723303854\n",
      "  (0, 217564)\t-0.04859364954906101\n",
      "  :\t:\n",
      "  (12006, 31739)\t0.030951656807556124\n",
      "  (12006, 30384)\t-0.12544010648852477\n",
      "  (12006, 26887)\t0.07663939610124455\n",
      "  (12006, 26755)\t0.0724274940051402\n",
      "  (12006, 25479)\t-0.07064487793904567\n",
      "  (12006, 24868)\t0.03736316375318835\n",
      "  (12006, 23881)\t-0.0620174772255501\n",
      "  (12006, 22336)\t0.07524025346868227\n",
      "  (12006, 21163)\t-0.08023475867097776\n",
      "  (12006, 20944)\t0.08023475867097776\n",
      "  (12006, 15816)\t0.08269194006725773\n",
      "  (12006, 14630)\t0.04080031865187836\n",
      "  (12006, 11574)\t0.11742109356558282\n",
      "  (12006, 10823)\t-0.1423879092550998\n",
      "  (12006, 10122)\t-0.08419912047582571\n",
      "  (12006, 9634)\t0.05659421816825786\n",
      "  (12006, 8924)\t0.06428387560778975\n",
      "  (12006, 8238)\t-0.07106099687617977\n",
      "  (12006, 7248)\t0.019617013991676098\n",
      "  (12006, 3807)\t-0.08138636101228366\n",
      "  (12006, 3261)\t-0.07345763740258773\n",
      "  (12006, 1907)\t-0.028377250202531182\n",
      "  (12006, 1220)\t0.08269194006725773\n",
      "  (12006, 739)\t-0.06589791302800656\n",
      "  (12006, 179)\t0.05553979621779862\n"
     ]
    }
   ],
   "source": [
    "data = sp.sparse.hstack((abstract, title, venue, year))\n",
    "X = abstract\n",
    "y = author_id\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271d94c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing data : 0.08196721311475409\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier(penalty = 'l2', loss = 'huber', learning_rate = 'adaptive', eta0 = 1, alpha = 0.001, random_state =42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Accuracy on testing data :\", model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "768b813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(abstract_test) # predicts the new values of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d769626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86e1aaa0c47659e08a896e9889384eb1e5401e6a</td>\n",
       "      <td>exploring,linear,subspace,hypothesis,gender,bi...</td>\n",
       "      <td>bolukbasi,et,al,2016,presents,one,first,gender...</td>\n",
       "      <td>2020</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>31461304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8d3076c38f56df22052567f4783c670d8e860f09</td>\n",
       "      <td>hyknow,end,end,task,oriented,dialog,modeling,h...</td>\n",
       "      <td>task,oriented,dialog,tod,systems,typically,man...</td>\n",
       "      <td>2021</td>\n",
       "      <td>FINDINGS</td>\n",
       "      <td>146950185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7c400ee676d427eeda1aad5c1c54c316f0b9773d</td>\n",
       "      <td>multilingual,information,extraction,pipeline,i...</td>\n",
       "      <td>introduce,advanced,information,extraction,pipe...</td>\n",
       "      <td>2018</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>1788050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185e7d2a761594451b02ace240356dadad2aef78</td>\n",
       "      <td>dice,loss,data,imbalanced,nlp,tasks</td>\n",
       "      <td>many,nlp,tasks,tagging,machine,reading,compreh...</td>\n",
       "      <td>2019</td>\n",
       "      <td>ACL</td>\n",
       "      <td>3043830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80</td>\n",
       "      <td>attention,guided,graph,convolutional,networks,...</td>\n",
       "      <td>dependency,trees,convey,rich,structural,inform...</td>\n",
       "      <td>2019</td>\n",
       "      <td>ACL</td>\n",
       "      <td>51044403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>069ebed0ba7adec30faa5c5e008053cf3eefc589</td>\n",
       "      <td>cas,french,corpus,clinical,cases</td>\n",
       "      <td>textual,corpora,extremely,important,various,nl...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Louhi@EMNLP</td>\n",
       "      <td>2105490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6527</th>\n",
       "      <td>b6e9fdc3e7bc4d379ee733b07199fe2a8336dd94</td>\n",
       "      <td>dependency,based,bilingual,language,models,reo...</td>\n",
       "      <td>paper,presents,novel,approach,improve,reorderi...</td>\n",
       "      <td>2014</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>143707112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528</th>\n",
       "      <td>5019da491732e412fafea4e1511818fd684cc1f1</td>\n",
       "      <td>complementary,strategies,low,resourced,morphol...</td>\n",
       "      <td>morphologically,rich,languages,challenging,nat...</td>\n",
       "      <td>2018</td>\n",
       "      <td></td>\n",
       "      <td>2814303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529</th>\n",
       "      <td>eca16c1c776406abd0d966653a705f945bd4b520</td>\n",
       "      <td>ungrammaticality,extra,grammaticality,natural,...</td>\n",
       "      <td>among,components,included,natural,language,und...</td>\n",
       "      <td>1979</td>\n",
       "      <td>ACL</td>\n",
       "      <td>3326473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6530</th>\n",
       "      <td>66b32bdb57d67bc10cb11605b5ba9bad8741789f</td>\n",
       "      <td>analysis,automatic,annotation,suggestions,hard...</td>\n",
       "      <td>many,complex,discourse,level,tasks,aid,domain,...</td>\n",
       "      <td>2019</td>\n",
       "      <td>ACL</td>\n",
       "      <td>153371561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6531 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paperId  \\\n",
       "0     86e1aaa0c47659e08a896e9889384eb1e5401e6a   \n",
       "1     8d3076c38f56df22052567f4783c670d8e860f09   \n",
       "2     7c400ee676d427eeda1aad5c1c54c316f0b9773d   \n",
       "3     185e7d2a761594451b02ace240356dadad2aef78   \n",
       "4     e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80   \n",
       "...                                        ...   \n",
       "6526  069ebed0ba7adec30faa5c5e008053cf3eefc589   \n",
       "6527  b6e9fdc3e7bc4d379ee733b07199fe2a8336dd94   \n",
       "6528  5019da491732e412fafea4e1511818fd684cc1f1   \n",
       "6529  eca16c1c776406abd0d966653a705f945bd4b520   \n",
       "6530  66b32bdb57d67bc10cb11605b5ba9bad8741789f   \n",
       "\n",
       "                                                  title  \\\n",
       "0     exploring,linear,subspace,hypothesis,gender,bi...   \n",
       "1     hyknow,end,end,task,oriented,dialog,modeling,h...   \n",
       "2     multilingual,information,extraction,pipeline,i...   \n",
       "3                   dice,loss,data,imbalanced,nlp,tasks   \n",
       "4     attention,guided,graph,convolutional,networks,...   \n",
       "...                                                 ...   \n",
       "6526                   cas,french,corpus,clinical,cases   \n",
       "6527  dependency,based,bilingual,language,models,reo...   \n",
       "6528  complementary,strategies,low,resourced,morphol...   \n",
       "6529  ungrammaticality,extra,grammaticality,natural,...   \n",
       "6530  analysis,automatic,annotation,suggestions,hard...   \n",
       "\n",
       "                                               abstract  year        venue  \\\n",
       "0     bolukbasi,et,al,2016,presents,one,first,gender...  2020        EMNLP   \n",
       "1     task,oriented,dialog,tod,systems,typically,man...  2021     FINDINGS   \n",
       "2     introduce,advanced,information,extraction,pipe...  2018        EMNLP   \n",
       "3     many,nlp,tasks,tagging,machine,reading,compreh...  2019          ACL   \n",
       "4     dependency,trees,convey,rich,structural,inform...  2019          ACL   \n",
       "...                                                 ...   ...          ...   \n",
       "6526  textual,corpora,extremely,important,various,nl...  2018  Louhi@EMNLP   \n",
       "6527  paper,presents,novel,approach,improve,reorderi...  2014        EMNLP   \n",
       "6528  morphologically,rich,languages,challenging,nat...  2018                \n",
       "6529  among,components,included,natural,language,und...  1979          ACL   \n",
       "6530  many,complex,discourse,level,tasks,aid,domain,...  2019          ACL   \n",
       "\n",
       "     prediction  \n",
       "0      31461304  \n",
       "1     146950185  \n",
       "2       1788050  \n",
       "3       3043830  \n",
       "4      51044403  \n",
       "...         ...  \n",
       "6526    2105490  \n",
       "6527  143707112  \n",
       "6528    2814303  \n",
       "6529    3326473  \n",
       "6530  153371561  \n",
       "\n",
       "[6531 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['prediction'] = y_pred.tolist() # puts the predicted data to list\n",
    "#final = test.set_axis(['paperId', 'title', 'abstract', 'year', 'venue', 'prediction'], axis=1, inplace=False) # changes the axis labels\n",
    "#final = final.drop(labels = ['title', 'year', 'abstract', 'venue'], axis = 1) # drops the labels column to get final result of only paperId & authorId\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f72495c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15161448      110\n",
       "1747849        86\n",
       "1683363        68\n",
       "2854981        66\n",
       "1750769        65\n",
       "             ... \n",
       "65737670        1\n",
       "1678747         1\n",
       "49276525        1\n",
       "2060291042      1\n",
       "153371561       1\n",
       "Name: prediction, Length: 1741, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55220e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the file into the predicted.json file required by teachers\n",
    "#output = final.to_dict(orient='records')\n",
    "#jsonString = json.dumps(output)\n",
    "#jsonFile = open('predicted.json', 'w')\n",
    "#jsonFile.write(jsonString)\n",
    "#jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb4c30f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "Process LokyProcess-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 890, in fit\n",
      "    return self._fit(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 686, in _fit\n",
      "    self._partial_fit(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 619, in _partial_fit\n",
      "    self._fit_multiclass(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 763, in _fit_multiclass\n",
      "    result = Parallel(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 456, in fit_binary\n",
      "    coef, intercept, average_coef, average_intercept, n_iter_ = _plain_sgd(\n",
      "KeyboardInterrupt\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 600, in __call__\n",
      "    raise WorkerInterrupt() from e\n",
      "joblib.my_exceptions.WorkerInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 439, in _process_worker\n",
      "    result_queue.put(_ResultItem(call_item.work_id, exception=exc))\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/queues.py\", line 241, in put\n",
      "    obj = dumps(obj, reducers=self._reducers)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 211, in __init__\n",
      "    loky_dt.update(_ReducerRegistry.dispatch_table)\n",
      "KeyboardInterrupt\n",
      "/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "Process LokyProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 890, in fit\n",
      "    return self._fit(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 686, in _fit\n",
      "    self._partial_fit(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 619, in _partial_fit\n",
      "    self._fit_multiclass(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 763, in _fit_multiclass\n",
      "    result = Parallel(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 456, in fit_binary\n",
      "    coef, intercept, average_coef, average_intercept, n_iter_ = _plain_sgd(\n",
      "KeyboardInterrupt\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 600, in __call__\n",
      "    raise WorkerInterrupt() from e\n",
      "joblib.my_exceptions.WorkerInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 439, in _process_worker\n",
      "    result_queue.put(_ResultItem(call_item.work_id, exception=exc))\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/queues.py\", line 241, in put\n",
      "    obj = dumps(obj, reducers=self._reducers)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 195, in __init__\n",
      "    loky_pickler_cls.__init__(self, writer, protocol=protocol)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 637, in __init__\n",
      "    Pickler.__init__(\n",
      "KeyboardInterrupt\n",
      "/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "Process LokyProcess-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 890, in fit\n",
      "    return self._fit(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 686, in _fit\n",
      "    self._partial_fit(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 619, in _partial_fit\n",
      "    self._fit_multiclass(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 763, in _fit_multiclass\n",
      "    result = Parallel(\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py\", line 456, in fit_binary\n",
      "    coef, intercept, average_coef, average_intercept, n_iter_ = _plain_sgd(\n",
      "KeyboardInterrupt\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 600, in __call__\n",
      "    raise WorkerInterrupt() from e\n",
      "joblib.my_exceptions.WorkerInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 439, in _process_worker\n",
      "    result_queue.put(_ResultItem(call_item.work_id, exception=exc))\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/queues.py\", line 241, in put\n",
      "    obj = dumps(obj, reducers=self._reducers)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 205, in __init__\n",
      "    loky_dt = dict(self.dispatch_table)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/_collections_abc.py\", line 720, in __iter__\n",
      "    yield from self._mapping\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/collections/__init__.py\", line 902, in __iter__\n",
      "    d.update(mapping)                   # reuses stored hash values if possible\n",
      "KeyboardInterrupt\n",
      "/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "Process LokyProcess-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 451, in _process_worker\n",
      "    _process_reference_size = _get_memory_usage(pid, force_gc=True)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 117, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "Process LokyProcess-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 451, in _process_worker\n",
      "    _process_reference_size = _get_memory_usage(pid, force_gc=True)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 117, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "Process LokyProcess-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 451, in _process_worker\n",
      "    _process_reference_size = _get_memory_usage(pid, force_gc=True)\n",
      "  File \"/Users/jasoncharnock/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 117, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py:434\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1753\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1753\u001b[0m \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:951\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m--> 951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mbackend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabort_everything\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     ensure_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m sgd \u001b[38;5;241m=\u001b[39m linear_model\u001b[38;5;241m.\u001b[39mSGDClassifier()\n\u001b[1;32m     17\u001b[0m random \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator\u001b[38;5;241m=\u001b[39msgd, param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m random_result \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Score: \u001b[39m\u001b[38;5;124m'\u001b[39m, random_result\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Params: \u001b[39m\u001b[38;5;124m'\u001b[39m, random_result\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:885\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimetric_:\n\u001b[1;32m    884\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_refit_for_multimetric(first_test_score)\n\u001b[0;32m--> 885\u001b[0m         refit_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# best_score_ iff refit is one of the scorer names\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# In single metric evaluation, refit_metric is \"score\"\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimetric_:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:729\u001b[0m, in \u001b[0;36mParallel.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[0;32m--> 729\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_terminate_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:759\u001b[0m, in \u001b[0;36mParallel._terminate_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_terminate_backend\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py:551\u001b[0m, in \u001b[0;36mLokyBackend.terminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;66;03m# Don't terminate the workers as we want to reuse them in later\u001b[39;00m\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;66;03m# calls, but cleanup the temporary resources that the Parallel call\u001b[39;00m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;66;03m# created. This 'hack' requires a private, low-level operation.\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temp_folder_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unlink_temporary_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_batch_stats()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_memmapping_reducer.py:612\u001b[0m, in \u001b[0;36mTemporaryResourcesManager._unlink_temporary_resources\u001b[0;34m(self, context_id)\u001b[0m\n\u001b[1;32m    607\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to delete temporary folder: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m                           \u001b[38;5;241m.\u001b[39mformat(pool_subfolder))\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalizers[context_id] \u001b[38;5;241m=\u001b[39m atexit\u001b[38;5;241m.\u001b[39mregister(_cleanup)\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unlink_temporary_resources\u001b[39m(\u001b[38;5;28mself\u001b[39m, context_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;124;03m\"\"\"Unlink temporary resources created by a process-based pool\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;66;03m# iterate over a copy of the cache keys because\u001b[39;00m\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;66;03m# unlink_temporary_resources further deletes an entry in this\u001b[39;00m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;66;03m# cache\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'huber', \n",
    "        'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1]\n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "l1 = np.arange(0,1,0.01)\n",
    "l2 = np.arange(0,1,0.01)\n",
    "class_weight = [{0:i,1:j} for i,j in zip(l1,l2)]\n",
    "eta0 = [0.1, 1, 10, 100]\n",
    "\n",
    "param_distributions = dict(loss=loss,\n",
    "                           penalty=penalty,\n",
    "                           alpha=alpha,\n",
    "                           learning_rate=learning_rate,\n",
    "                           eta0=eta0)\n",
    "sgd = linear_model.SGDClassifier()\n",
    "random = RandomizedSearchCV(estimator=sgd, param_distributions=param_distributions, verbose=1, n_jobs=-1, n_iter=50, cv=3)\n",
    "random_result = random.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score: ', random_result.best_score_)\n",
    "print('Best Params: ', random_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2783b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1 # the amount of times that it will run\n",
    "batchsize = 10000 # the amount of data you put in per batch\n",
    "model = SGDClassifier(penalty = 'l2', loss = 'huber', learning_rate = 'adaptive', eta0 = 1, alpha = 0.001, random_state = 42) # The classifier to use, this is best applicable to large datasets\n",
    "batches = int(X_train.shape[0]/batchsize) + 1\n",
    "samples = X_train.shape[0]\n",
    "for i in range(epoch):\n",
    "    for j in range(batches):\n",
    "        print('in j...', j, j*batchsize, '----2is:',samples, (j+1)*batchsize )\n",
    "        model.partial_fit(X_train[j*batchsize:min(samples,(j+1)*batchsize)], \n",
    "                          y_train[j*batchsize:min(samples,(j+1)*batchsize)], \n",
    "                          classes=np.unique(y))\n",
    "print(\"Done\")\n",
    "print (\"Accuracy on testing data :\", model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b253d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  We are convinced that this is a classification problem. Is there any way to approach this with regression? \n",
    "# Or is that only possible for binary classification\n",
    "\n",
    "#2. Does it make sense to oversample the data that only occurs once? \n",
    "# Since we will split the data and this dataset is quite unbalanced, we will get unknown authors in the \n",
    "# validation set that are not trained yet. But when we read about oversampling they say that \n",
    "# this should be done after splitting\n",
    "\n",
    "#3. Will it improve performance to add columns such as abstract length, amount of stopwords used, average word count, etc?\n",
    "\n",
    "# 4. Is it adviced to use the tfidf or the countvectorizer? \n",
    "\n",
    "# 5. Is there anything we could improve for the preprocessing of the text? \n",
    "# We have already normalized, lemmatized and tokenized the text\n",
    "\n",
    "# 6. Are we required to use BERT to get an accuracy above 15? This seems more like a deep learning method than ML\n",
    "\n",
    "# 7. Wouldn't it be better to use all our training data to train for the test data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
